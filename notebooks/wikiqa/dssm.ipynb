{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/NTMC-Community/MatchZoo/blob/2.0/artworks/matchzoo-logo.png?raw=True\" alt=\"logo\" style=\"width:600px;float: center\"/>\n",
    "\n",
    "### Build a Deep Semantic Structured Model (DSSM)\n",
    "\n",
    "This is a tutorial on training *Deep Semantic Similarity Model* [Huang et al. 2013](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DSSM_cikm13_talk_v4.pdf) model with [MatchZoo](https://github.com/faneshion/MatchZoo). We use [WikiQA](https://aclweb.org/anthology/D15-1237) as the example benchmark data set to show the usage.\n",
    "\n",
    "Features:\n",
    "\n",
    "1. Using the tri-letter based word hashing for scalable word representation.\n",
    "2. Using the deep neural net to extract high-level semantic representations.\n",
    "3. Using the click signal to guide the learning.\n",
    "\n",
    "\n",
    "\n",
    "*To walk through this notebook, you need approx 30 minutes.*\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MatchZoo expect a list of *Quintuple* as training data:\n",
    "\n",
    "```python\n",
    "train = [('qid0', 'did0', 'query 0', 'document 0', 'label 0'),\n",
    "         ('qid0', 'did1', 'query 0', 'document 1', 'label 1'),\n",
    "          ...,\n",
    "         ('qid1', 'did2', 'query 1', 'document 2', 'label 3')]\n",
    "```\n",
    "\n",
    "The corresponded columns are `(text_left_id, text_right_id, text_left, text_right, label)`. For Information Retrieval task, *text_left* is referred as *query*, and *text_right* is document.\n",
    "\n",
    "For the test case, MatchZoo expect a list of *Quadruple* (we do not need labels) as input:\n",
    "\n",
    "```python\n",
    "test = [('qid9', 'did5', 'query 9', 'document 5'),\n",
    "         ...,\n",
    "        ('qid2', 'did7', 'query 2', 'document 7')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "+ Prepare WikiQA dataset\n",
    "    - Download\n",
    "    - Load\n",
    "    - Adjustment\n",
    "+ Preprocessing\n",
    "+ Data Generator\n",
    "+ Model Training\n",
    "    - Initialize\n",
    "    - Hyper-Parameters\n",
    "    - Make Prediction\n",
    "    - Model Persistence\n",
    "- Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare WikiQA dataset\n",
    "\n",
    "#### Download\n",
    "\n",
    "We take WikiQA as the example benchmark dataset to show the usage of MatchZoo. Firstly you need to downlowd the data and uncompress the data, we provided the following script to help you download the dataset into `MatchZoo/data/WikiQA` folder, you can change the directory in the following script.\n",
    "\n",
    "If you already have WikiQA dataset downloaded on your machine, skip the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:56:56.779280Z",
     "start_time": "2018-11-05T09:56:53.136182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiQA already exists, download again?(Y/N)N\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "if not Path('../../data/WikiQA').exists():\n",
    "    !mkdir -p ../../data/WikiQA\n",
    "    !wget -P ../../data/WikiQA https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
    "    !unzip -d -o ../../data/WikiQA ../../data/WikiQA/WikiQACorpus.zip\n",
    "elif input('WikiQA already exists, download again?(Y/N)').lower() == 'y':\n",
    "    !rm -rf ../../data/WikiQA/\n",
    "    !mkdir -p ../../data/WikiQA\n",
    "    !wget -P ../../data/WikiQA https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
    "    !unzip -o -d  ../../data/WikiQA ../../data/WikiQA/WikiQACorpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Adjustment\n",
    "\n",
    "The *train/dev/test* files of WikiQA are *WikiQA-train.tsv*, *WikiQA-dev.tsv*, *WikiQA-test.tsv* under the uncompressed folder WikiQACorpus. The data format of WikiQA is as follows:\n",
    "\n",
    "`QuestionID\\tQuestion\\tDocumentID\\tDocumentTitle\\tSentenceID\\tSentence\\tLabel`\n",
    "\n",
    "We can convert this format to the expected input format of MatchZoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:56:56.844078Z",
     "start_time": "2018-11-05T09:56:56.780987Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(path, stage):\n",
    "    def scan_file():\n",
    "        with open(path) as in_file:\n",
    "            next(in_file)  # skip header\n",
    "            for l in in_file:\n",
    "                yield l.strip().split('\\t')\n",
    "    if stage == 'train':\n",
    "        return [(qid, did, q, d, label) for qid, q, _, _, did, d, label in scan_file()]\n",
    "    elif stage == 'test':\n",
    "        return [(qid, did, q, d) for qid, q, _, _, did, d, _ in scan_file()]\n",
    "\n",
    "train = read_data('../../data/WikiQA/WikiQACorpus/WikiQA-train.tsv', stage='train')\n",
    "test  = read_data('../../data/WikiQA/WikiQACorpus/WikiQA-test.tsv', stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:56:56.854945Z",
     "start_time": "2018-11-05T09:56:56.845936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Q1',\n",
       " 'D1-0',\n",
       " 'how are glacier caves formed?',\n",
       " 'A partly submerged glacier cave on Perito Moreno Glacier .',\n",
       " '0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:56:56.862133Z",
     "start_time": "2018-11-05T09:56:56.857012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Q0',\n",
       " 'D0-0',\n",
       " 'HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US',\n",
       " 'African immigration to the United States refers to immigrants to the United States who are or were nationals of Africa .')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "You can pre-process your DSSM input in three lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.361851Z",
     "start_time": "2018-11-05T09:56:56.864469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Start building vocabulary & fitting parameters.\n",
      "2118it [00:00, 4066.81it/s]\n",
      "18841it [00:06, 2694.41it/s]\n",
      "Start processing input data for train stage.\n",
      "2118it [00:00, 3254.63it/s]\n",
      "18841it [00:09, 2041.38it/s]\n",
      "Start processing input data for predict stage.\n",
      "633it [00:00, 3134.98it/s]\n",
      "5961it [00:03, 1985.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dssm preprocessor.\n",
    "from matchzoo import preprocessor\n",
    "dssm_preprocessor = preprocessor.DSSMPreprocessor()\n",
    "datapack_train = dssm_preprocessor.fit_transform(train, stage='train')\n",
    "datapack_test = dssm_preprocessor.fit_transform(test, stage='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `processed_tr`?**\n",
    "\n",
    "`processed_tr` is a **MatchZoo DataPack** data structure (see `matchzoo/datapack.py`). It contains \n",
    "1. A *2-columns* `pandas DataFrame` called `left` to host all the pre-processed records including index and processed text to store `text_left` and `id_left`.\n",
    "2. A *2-columns* `pandas DataFrame` called `right` to host all the pre-processed records including index and processed text to store `text_right` and `id_right`.\n",
    "3. A *2-columns* `pandas DataFrame` called `relation` to host all the pre-processed records including index and index mapping `id_left` and `id_right`.\n",
    "4. A `context` property (dictionary) consists of all the parameters fitted during pre-processing. \n",
    "\n",
    "The `fit_transform` method is a linear combination of two methods:\n",
    "\n",
    "1. Fit parameters using the `fit` function, this only happens when `stage='train'`.\n",
    "2. Transform data into expected format.\n",
    "\n",
    "So the previous three lines code can also be written as:\n",
    "\n",
    "```python\n",
    "# Initialize a dssm preprocessor.\n",
    "from matchzoo import preprocessor\n",
    "dssm_preprocessor = preprocessor.DSSMPreprocessor()\n",
    "processed_tr = dssm_preprocessor.fit_transform(train, stage='train')\n",
    "# We do not need to fit any parameters during the testing stage.\n",
    "# So we can call transform directly.\n",
    "processed_te = dssm_preprocessor.transform(test, stage='test')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described, the fitted parameters were stored in `context` property, to access the context, just call:\n",
    "\n",
    "```python\n",
    "print(processed_tr.context)\n",
    "```\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.366108Z",
     "start_time": "2018-11-05T09:57:19.363403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  9643\n"
     ]
    }
   ],
   "source": [
    "print('vocab size: ', len(datapack_train.context['term_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What has been stored in the `context?`** \n",
    "\n",
    "We stored `input_shapes` in the context property. Since DSSM model's model input shape is dynamic (it depends on user's training data to generate tri-letters), so you **must** manually set models input shape, we'll discuss it in the model training section.\n",
    "\n",
    "**What is `dssm_preprocessor` actually doing?**\n",
    "\n",
    "The `dssm_preprocessor` is calling a sequence of `process_units`. Each `process_unit` is designed to perform one atom operation on input data. For instance, in `dssm_preprocessor`, we called:\n",
    "\n",
    "1. TokenizeUnit: Perform tokenization on raw input data.\n",
    "2. LowercaseUnit: Transform all tokens into lower case.\n",
    "3. PuncRemovalUnit: Remove all the punctuations.\n",
    "4. StopRemovalUnit: Remove all the stopwords.\n",
    "5. NgramLetterUnit: Create n-gram-letters (by default we're creating tri-letters) as input data, for example: the token `test` we be transformed to `['#te', 'tes', 'est', 'st#']`.\n",
    "6. VocabularyUnit: Create vocabulary to get the dimensionality of `tri-letters`.\n",
    "7. WordHashingUnit: Create `WordHashing` layer as described in the paper.\n",
    "\n",
    "----\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "For memory efficiency, we expect you to use **generator** to generate batches of data on the fly. For example, we can create a **PointGenerator** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.371266Z",
     "start_time": "2018-11-05T09:57:19.367815Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matchzoo import generators\n",
    "from matchzoo import tasks\n",
    "generator_train = generators.PointGenerator(inputs=datapack_train, task=tasks.Ranking(), batch_size=64, stage='train')\n",
    "generator_test = generators.PointGenerator(inputs=datapack_test, task=tasks.Ranking(), batch_size=64, stage='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first batch of trainig data, just call `X_train, y_train = generator[0]`.\n",
    "\n",
    "**What is PointGenerator?**\n",
    "**PointGenerator** is this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the problem can be approximated by a regression/Classification problem — given a single query-document pair, predict its score.\n",
    "\n",
    "A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.\n",
    "\n",
    "**What is PairGenerator?**\n",
    "In this case, the problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents.\n",
    "\n",
    "In MatchZoo, **PairGenerator** generate one positive & `num_neg` negative examples per pair. As an example, to train a DSSM model (for document ranking), we use `num_neg=4`. \n",
    "\n",
    "**What is ListGenerator?**\n",
    "This generator try to directly optimize the value of evaluation measures, averaged over all queries in the training data. \n",
    "\n",
    "Chosse the appropriate generator based on your `task`.\n",
    "\n",
    "----\n",
    "\n",
    "### Train Your DSSM Model\n",
    "\n",
    "To train a DSSM model, we need to create an instance of DSSMModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.377321Z",
     "start_time": "2018-11-05T09:57:19.373340Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matchzoo import models\n",
    "dssm_model = models.DSSMModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to set hyper-parameters to our DSSM Model. In general, there are **two types of hyper-parameters**:\n",
    "\n",
    "**Required parameters**: For DSSM, since the `input_shapes` depend on the dimensionality of fitted training data, you're required to set this parameter manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.383677Z",
     "start_time": "2018-11-05T09:57:19.379233Z"
    }
   },
   "outputs": [],
   "source": [
    "# The fitted parameters is stored in the `context` property of pre-processor instance during the training stage.\n",
    "from matchzoo import losses\n",
    "from matchzoo import tasks\n",
    "input_shapes = datapack_train.context['input_shapes']\n",
    "dssm_model.params['input_shapes'] = input_shapes\n",
    "dssm_model.params['task'] = tasks.Ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.390617Z",
     "start_time": "2018-11-05T09:57:19.385814Z"
    }
   },
   "outputs": [],
   "source": [
    "dssm_model.params['task'].metrics = ['mae', 'map']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as **required parameters**, use `dssm_model.params['parameter-name'] = parameter-value` to set the hyper parameters. If you want to keep everything by default values, just use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:19.398913Z",
     "start_time": "2018-11-05T09:57:19.392809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dssm parameters:  name                          DSSMModel\n",
      "model_class                   <class 'matchzoo.models.dssm_model.DSSMModel'>\n",
      "input_shapes                  [(9644,), (9644,)]\n",
      "task                          <matchzoo.tasks.ranking.Ranking object at 0x125e3c358>\n",
      "optimizer                     adam\n",
      "w_initializer                 glorot_normal\n",
      "b_initializer                 zeros\n",
      "dim_fan_out                   128\n",
      "dim_hidden                    300\n",
      "activation_hidden             tanh\n",
      "num_hidden_layers             2\n"
     ]
    }
   ],
   "source": [
    "dssm_model.guess_and_fill_missing_params()\n",
    "print('dssm parameters: ', dssm_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "To train the model after all the parameters were settled, call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:22.719090Z",
     "start_time": "2018-11-05T09:57:19.401688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "20/20 [==============================] - 2s 77ms/step - loss: 0.1460 - mean_absolute_error: 0.2631\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 0.1346 - mean_absolute_error: 0.2585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125e42358>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dssm_model.build()\n",
    "dssm_model.compile()\n",
    "dssm_model.fit_generator(generator_train, steps_per_epoch=20, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:22.737134Z",
     "start_time": "2018-11-05T09:57:22.720625Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = generator_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:22.869876Z",
     "start_time": "2018-11-05T09:57:22.739242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "64/64 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.14020322263240814,\n",
       " 'mean_absolute_error': 0.2513119578361511,\n",
       " 'mean_average_precision(0)': 0.06349206349206349}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dssm_model.evaluate(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:57:23.233671Z",
     "start_time": "2018-11-05T09:57:22.878317Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-42ba6d85b9d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mid_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{} is predicted as {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_te' is not defined"
     ]
    }
   ],
   "source": [
    "for id_left, id_right, pred in zip(X_te.id_left, X_te.id_right, predictions):\n",
    "    print(\"{}/{} is predicted as {}\".format(id_left, id_right, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Persistence\n",
    "\n",
    "You can persist your trained model using `model.save()` and `load_model` function:\n",
    "\n",
    "```python\n",
    "from matchzoo import engine\n",
    "# Save the model to dir.\n",
    "dssm_model.save('/your-model-saved-path')\n",
    "# And load the model from dir.\n",
    "engine.load_model('/your-model-saved-path')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Huang et al. 2013] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proc. CIKM. ACM, 2333–2338."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "matchzoo",
   "language": "python",
   "name": "matchzoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
